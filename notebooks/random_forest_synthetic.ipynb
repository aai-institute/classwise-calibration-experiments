{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forest calibration experiment on Synthetic Data\n",
    "\n",
    "We compare multiple calibration algorithms with their reduced, \n",
    "class-wise and class-wise reduced counterparts.\n",
    "\n",
    "We work with a random forest classifier trained on two synthetic datasets each containing\n",
    "5 classes and 60k samples, where one of them is imbalanced.\n",
    "\n",
    "The model is trained on 30k samples (from a stratified shuffle split)\n",
    "and achieves an accuracy of roughly 89% in both cases.\n",
    "\n",
    "As is common with random forests, the resulting model is highly miscalibrated\n",
    "(pre-calibration ECE â‰ˆ 0.23, post-calibration ECE < 0.03).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from kyle.evaluation import EvalStats\n",
    "\n",
    "from src.constants import OUTPUT_DIR, RANDOM_SEED\n",
    "from src.utils import (\n",
    "    configure_plots,\n",
    "    perform_default_evaluation,\n",
    "    plot_evaluation_results_from_dataframe,\n",
    "    set_random_seed,\n",
    ")\n",
    "from src.utils.evaluation import combined_results_into_dataframe\n",
    "from src.utils.other import get_rf_calibration_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = OUTPUT_DIR / \"random_forest_synthetic\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "IMBALANCED_WEIGHTS = (0.3, 0.1, 0.25, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seed(RANDOM_SEED)\n",
    "configure_plots()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Creating balanced dataset\")\n",
    "balanced_confs, balanced_gt = get_rf_calibration_dataset()\n",
    "\n",
    "logger.info(f\"Creating imbalanced dataset\")\n",
    "imbalanced_confs, imbalanced_gt = get_rf_calibration_dataset(weights=IMBALANCED_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Calibration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_stats = EvalStats(balanced_gt, balanced_confs, bins=25)\n",
    "logger.info(\n",
    "    f\"Balanced ECE before calibration: {eval_stats.expected_calibration_error()}\"\n",
    ")\n",
    "\n",
    "eval_stats = EvalStats(imbalanced_gt, imbalanced_confs, bins=25)\n",
    "logger.info(\n",
    "    f\"Imbalanced ECE before calibration: {eval_stats.expected_calibration_error()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalibration\n",
    "We evaluate reduction wrappers on multiple metrics with different calibration algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Performing evaluation for balanced dataset\")\n",
    "\n",
    "balanced_eval_results = perform_default_evaluation(\n",
    "    confidences=balanced_confs,\n",
    "    gt_labels=balanced_gt,\n",
    ")\n",
    "\n",
    "rf_balanced_results_df = combined_results_into_dataframe(\n",
    "    balanced_eval_results,\n",
    "    model_name=\"Random Forest\",\n",
    "    dataset_name=\"Synthetic Balanced\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_methods_order: List[str] = (\n",
    "    rf_balanced_results_df[\"Reduction Method\"].unique().tolist()\n",
    ")\n",
    "reduction_methods_order = [reduction_methods_order[0]] + sorted(\n",
    "    reduction_methods_order[1:], key=len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Performing evaluation for imbalanced dataset\")\n",
    "\n",
    "imbalanced_eval_results = perform_default_evaluation(\n",
    "    confidences=imbalanced_confs,\n",
    "    gt_labels=imbalanced_gt,\n",
    ")\n",
    "\n",
    "rf_imbalanced_results_df = combined_results_into_dataframe(\n",
    "    imbalanced_eval_results,\n",
    "    model_name=\"Random Forest\",\n",
    "    dataset_name=\"Synthetic Imbalanced\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([rf_balanced_results_df, rf_imbalanced_results_df])\n",
    "logger.info(\"Saving results\")\n",
    "output_file = output_dir / \"results.csv\"\n",
    "results_df.to_csv(output_file, sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Plotting results\")\n",
    "\n",
    "results_df = results_df.query(\"(Metric != 'condition') & (Metric != 'weak_condition')\")\n",
    "\n",
    "plot_evaluation_results_from_dataframe(\n",
    "    results_df.query(\n",
    "        \"(Dataset == 'Synthetic Balanced') \"\n",
    "        \"& (`Calibration Method` != 'TemperatureScaling') \"\n",
    "    ),\n",
    "    hue_order=reduction_methods_order,\n",
    "    output_file=(output_dir / \"evaluation_ECE_rf_balanced.eps\"),\n",
    "    show=False,\n",
    ")\n",
    "\n",
    "plot_evaluation_results_from_dataframe(\n",
    "    results_df.query(\n",
    "        \"(Dataset == 'Synthetic Imbalanced') \"\n",
    "        \"& (`Calibration Method` != 'TemperatureScaling') \"\n",
    "    ),\n",
    "    hue_order=reduction_methods_order,\n",
    "    output_file=(output_dir / \"evaluation_ECE_rf_balanced.eps\"),\n",
    "    show=False,\n",
    ")\n",
    "\n",
    "plot_evaluation_results_from_dataframe(\n",
    "    results_df.query(\"(`Calibration Method` == 'TemperatureScaling') \"),\n",
    "    hue_order=reduction_methods_order,\n",
    "    output_file=(output_dir / \"evaluation_ECE_rf_temperature_scaling.eps\"),\n",
    "    show=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
